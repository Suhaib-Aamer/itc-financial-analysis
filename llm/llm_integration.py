# -*- coding: utf-8 -*-
"""LLM Integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1INzIKZYbdet8pev5Ts5ilYTtu2rEdNXL
"""

pip install -q langchain chromadb langchain-community sentence-transformers langchain_google_genai

from sentence_transformers import SentenceTransformer
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

from langchain_google_genai import GoogleGenerativeAI

from langchain_core.messages import HumanMessage, AIMessage

from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain.chains import LLMChain

from langchain.schema.output_parser import StrOutputParser
from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder
from langchain_google_genai import ChatGoogleGenerativeAI

# Unzip the folder after uploading
import zipfile

with zipfile.ZipFile('chroma_db_32_backup.zip', 'r') as zip_ref:
    zip_ref.extractall('chroma_db')

embedding = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')

# Creating the vector database
vectorstore = Chroma(
    persist_directory='chroma_db',
    embedding_function=embedding
)

# Creating a retriever
mmr_retriever = vectorstore.as_retriever(
    search_type = "mmr",
    search_kwargs = {"k":3, "lambda_mult":1}
)

from google.colab import userdata
import os

gk = userdata.get('google_key')

llm = GoogleGenerativeAI(api_key = gk, model = 'gemini-2.0-flash', temperature=1)

# Defining the retriever + formatter
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def get_docs_and_context(question):
    docs = mmr_retriever.get_relevant_documents(question)
    return {"input": input, "docs": docs, "context": format_docs(docs)}

# Parallel chain gets all pieces: question, context, docs
parallel_chain = RunnableLambda(lambda x: {
    "input": x["input"],
    **get_docs_and_context(x["input"])
})

# Creating a memory buffer
memory_buffer = {"chat_history":[]}

def get_history_from_buffer(humna_input):
  return memory_buffer['chat_history']

runnable_get_history_from_buffer = RunnableLambda(get_history_from_buffer)

# Chat Prompt Template
chat_prompt = ChatPromptTemplate.from_messages([
    ("system",
     """
     You are a domain-specific AI financial analyst focused on company-level performance evaluation.

     Your task is to analyze and respond to user financial queries *strictly based on the provided transcript data*: {context}.

     Rules:
     1. ONLY extract facts, figures, and insights that are explicitly available in the transcript.
     2. If data is *missing or partially available*, clearly state: "The required data is not available in the current transcript." Then provide a generic but relevant explanation based on standard financial principles.
     3. Maintain numerical accuracy and avoid interpretation beyond data boundaries.
     4. Prioritize answers relevant to *ITC Ltd.*, but keep response format adaptable to other firms and fiscal years.
     5. Clearly present year-wise or metric-wise insights using bullet points or structured formats if applicable.

     Your goals:
     - Ensure 100% fidelity to source transcript.
     - Do not assume or hallucinate missing numbers.
     - Use clear, reproducible reasoning steps (e.g., show which line items support your conclusion).
     - Output should be modular enough to scale across other companies and time periods.

     Respond only to this question from the user.
     """),
    MessagesPlaceholder(variable_name="chat_history", optional = True),

    ("human", "{input}")
])

# Initializing the Gemini LLM and parser
llm = ChatGoogleGenerativeAI(
    api_key=gk,
    model="gemini-2.0-flash-exp",
    temperature=1
)

parser = StrOutputParser()

# Composing the final chain

main_chain = (
    parallel_chain |
    # Separate context/question for prompt, and carry docs separately
    RunnableLambda(lambda x: {
        "llm_input": {"input": x["input"], "context": x["context"]},
        "docs": x["docs"]
    }) |

    # Generate answer from prompt/LLM
    RunnableLambda(lambda x: {
        "result": (chat_prompt | llm | parser).invoke(x["llm_input"]),
        "source_documents": x["docs"]
    })
)

# This is the final chain
chain = RunnablePassthrough.assign(chat_history = runnable_get_history_from_buffer) | main_chain

query = "What was ITCâ€™s profitability?"
output = chain.invoke({"input": query})

print("Answer:")
print(output["result"])
print("\nSources:")
for doc in output["source_documents"]:
    print(doc.metadata)

# Conversational Chat-Bot

while True:
    query = input("*Human Input: ")

    if query.lower() in ['bye', 'exit', 'quit']:
        print("Bye bye")
        break

    output = chain.invoke({"input": query})

    print("Answer:")
    print(output["result"])

    print("\nSources:")
    for doc in output.get("source_documents", []):
        print(doc.metadata)

    memory_buffer['chat_history'].append(HumanMessage(content=query))
    memory_buffer['chat_history'].append(AIMessage(content=output["result"]))

